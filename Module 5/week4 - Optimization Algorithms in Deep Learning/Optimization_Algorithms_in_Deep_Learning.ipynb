{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "u6oZgzPihb7k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent"
      ],
      "metadata": {
        "id": "P3ZizuAPfUy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(W):\n",
        "  \"\"\"\n",
        "  Thực hiện tính gradient của dw1 và dw2\n",
        "  Arguments:\n",
        "  W -- np.array [w1, w2]\n",
        "  Returns:\n",
        "  dW -- np.array [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2\n",
        "  \"\"\"\n",
        "  w1, w2 = W\n",
        "  dw1 = 0.2 * w1\n",
        "  dw2 = 4 * w2\n",
        "  dW = np.array([dw1, dw2])\n",
        "  return dW\n",
        "\n",
        "\n",
        "def sgd(W, dW, lr):\n",
        "  \"\"\"\n",
        "  Thực hiện thuật toán Gradient Descent để update w1 và w2\n",
        "  Arguments:\n",
        "  W -- np.array [w1, w2]\n",
        "  dW -- np.array [dw1, dw2], array chứa giá trị đạo hàm theo w1 và w2\n",
        "  lr -- float: learning rate\n",
        "  Returns:\n",
        "  W -- np.array [w1, w2] w1 và w2 sau khi update\n",
        "  \"\"\"\n",
        "  W = W - lr * dW\n",
        "  return W\n",
        "\n",
        "\n",
        "def train_p1(optimizer, lr, epochs):\n",
        "  \"\"\"\n",
        "  Thực hiện tìm điểm minium của function (1) dựa vào thuật toán được\n",
        "  truyền vào từ optimizer\n",
        "  Arguments:\n",
        "  optimize: function thực hiện thuật toán optimization cụ thể\n",
        "  lr -- float: learning rate\n",
        "  epochs -- int: số lượng lần (epoch) lặp để tìm minium\n",
        "  Returns:\n",
        "  results -- list: list các cặp điểm [w1, w2] sau mỗi epoch (mỗi lần cập nhật)\n",
        "  \"\"\"\n",
        "\n",
        "  # initial point\n",
        "  W = np.array([-5, -2], dtype=np.float32)\n",
        "  # list of results\n",
        "  results = [W]\n",
        "\n",
        "  # Tạo vòng lặp theo số epochs\n",
        "  # Tìm gradient dW gồm dw1 và dw2\n",
        "  # dùng thuật toán optimization cập nhật w1, w2\n",
        "  # append cặp [w1, w2] vào list results\n",
        "  for i in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W = optimizer(W, dW, lr)\n",
        "    results.append(W)\n",
        "    print(f'Epoch {i + 1}: w1 = {W[0]}, w2 = {W[1]}')\n",
        "  return results"
      ],
      "metadata": {
        "id": "0p-tjAnBfGOe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sgd = train_p1(sgd, 0.4, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZagRrujJgfnI",
        "outputId": "6ee7cdb3-221f-4141-dc66-993ec4162d1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w1 = -4.6, w2 = 1.2000000000000002\n",
            "Epoch 2: w1 = -4.231999999999999, w2 = -0.7200000000000002\n",
            "Epoch 3: w1 = -3.893439999999999, w2 = 0.43200000000000016\n",
            "Epoch 4: w1 = -3.5819647999999993, w2 = -0.2592000000000001\n",
            "Epoch 5: w1 = -3.2954076159999994, w2 = 0.1555200000000001\n",
            "Epoch 6: w1 = -3.0317750067199993, w2 = -0.09331200000000006\n",
            "Epoch 7: w1 = -2.7892330061823993, w2 = 0.05598720000000004\n",
            "Epoch 8: w1 = -2.5660943656878072, w2 = -0.03359232000000004\n",
            "Epoch 9: w1 = -2.360806816432783, w2 = 0.020155392000000022\n",
            "Epoch 10: w1 = -2.1719422711181604, w2 = -0.012093235200000017\n",
            "Epoch 11: w1 = -1.9981868894287076, w2 = 0.007255941120000012\n",
            "Epoch 12: w1 = -1.838331938274411, w2 = -0.0043535646720000085\n",
            "Epoch 13: w1 = -1.691265383212458, w2 = 0.0026121388032000056\n",
            "Epoch 14: w1 = -1.5559641525554613, w2 = -0.0015672832819200039\n",
            "Epoch 15: w1 = -1.4314870203510244, w2 = 0.0009403699691520025\n",
            "Epoch 16: w1 = -1.3169680587229424, w2 = -0.0005642219814912016\n",
            "Epoch 17: w1 = -1.211610614025107, w2 = 0.00033853318889472107\n",
            "Epoch 18: w1 = -1.1146817649030984, w2 = -0.00020311991333683268\n",
            "Epoch 19: w1 = -1.0255072237108505, w2 = 0.00012187194800209963\n",
            "Epoch 20: w1 = -0.9434666458139824, w2 = -7.312316880125978e-05\n",
            "Epoch 21: w1 = -0.8679893141488638, w2 = 4.387390128075587e-05\n",
            "Epoch 22: w1 = -0.7985501690169547, w2 = -2.6324340768453522e-05\n",
            "Epoch 23: w1 = -0.7346661554955983, w2 = 1.5794604461072116e-05\n",
            "Epoch 24: w1 = -0.6758928630559504, w2 = -9.476762676643271e-06\n",
            "Epoch 25: w1 = -0.6218214340114744, w2 = 5.686057605985963e-06\n",
            "Epoch 26: w1 = -0.5720757192905565, w2 = -3.4116345635915786e-06\n",
            "Epoch 27: w1 = -0.526309661747312, w2 = 2.046980738154947e-06\n",
            "Epoch 28: w1 = -0.484204888807527, w2 = -1.2281884428929686e-06\n",
            "Epoch 29: w1 = -0.44546849770292485, w2 = 7.36913065735781e-07\n",
            "Epoch 30: w1 = -0.40983101788669085, w2 = -4.4214783944146867e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent + Momentum"
      ],
      "metadata": {
        "id": "1EFoIxp2fTp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(W):\n",
        "  w1, w2 = W\n",
        "  dw1 = 0.2 * w1\n",
        "  dw2 = 4 * w2\n",
        "  dW = np.array([dw1, dw2])\n",
        "  return dW\n",
        "\n",
        "\n",
        "def sgd_momentum(W, dW, lr, beta, V):\n",
        "  V = beta * V + (1 - beta) * dW\n",
        "  W = W - lr * V\n",
        "  return W, V\n",
        "\n",
        "\n",
        "def train_p2(optimizer, lr, beta, epochs):\n",
        "  W = np.array([-5, -2], dtype=np.float32)\n",
        "  V = np.zeros_like(W)\n",
        "  results = [W]\n",
        "  for i in range(epochs):\n",
        "      dW = df_w(W)\n",
        "      W, V = optimizer(W, dW, lr, beta, V)\n",
        "      results.append(W)\n",
        "      print(f'Epoch {i + 1}: w1 = {W[0]}, w2 = {W[1]}')\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "IgeEmttWfhB3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sgd_momentum = train_p2(sgd_momentum, 0.6, 0.5, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOFKTa9igc0k",
        "outputId": "e2f15a74-41ee-4587-e8f7-15f25fc5777f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w1 = -4.7, w2 = 0.3999999999999999\n",
            "Epoch 2: w1 = -4.268, w2 = 1.12\n",
            "Epoch 3: w1 = -3.7959199999999997, w2 = 0.13600000000000012\n",
            "Epoch 4: w1 = -3.3321248, w2 = -0.5192\n",
            "Epoch 5: w1 = -2.900299712, w2 = -0.22376000000000013\n",
            "Epoch 6: w1 = -2.5103691852799996, w2 = 0.19247199999999992\n",
            "Epoch 7: w1 = -2.1647817708031996, w2 = 0.16962160000000004\n",
            "Epoch 8: w1 = -1.8621011573166075, w2 = -0.04534951999999995\n",
            "Epoch 9: w1 = -1.599034781134315, w2 = -0.09841565599999999\n",
            "Epoch 10: w1 = -1.3715595061751098, w2 = -0.0068499368000000255\n",
            "Epoch 11: w1 = -1.1755282983250006, w2 = 0.04715284695999999\n",
            "Epoch 12: w1 = -1.006980996500446, w2 = 0.01757082248800001\n",
            "Epoch 13: w1 = -0.8622884857981419, w2 = -0.018305176733599993\n",
            "Epoch 14: w1 = -0.7382049212991013, w2 = -0.01427696426408\n",
            "Epoch 15: w1 = -0.6318708437716349, w2 = 0.004869499087575998\n",
            "Epoch 16: w1 = -0.5407915543816036, w2 = 0.0085993318583128\n",
            "Epoch 17: w1 = -0.4628044164236918, w2 = 0.00014505001370584102\n",
            "Epoch 18: w1 = -0.39604258245931434, w2 = -0.004256150925044647\n",
            "Epoch 19: w1 = -0.3388991105295668, w2 = -0.0013493702843663147\n",
            "Epoch 20: w1 = -0.289993427932919, w2 = 0.0017232643772124292\n",
            "Epoch 21: w1 = -0.24814098095861994, w2 = 0.0011916644553468863\n",
            "Epoch 22: w1 = -0.21232629861395325, w2 = -0.0005041328520021488\n",
            "Epoch 23: w1 = -0.1816793795247827, w2 = -0.0007470720832740878\n",
            "Epoch 24: w1 = -0.15545515720871045, w2 = 2.7944801018848055e-05\n",
            "Epoch 25: w1 = -0.1330157366181517, w2 = 0.00038191948194269836\n",
            "Epoch 26: w1 = -0.11381508212578322, w2 = 0.00010060344407338548\n",
            "Epoch 27: w1 = -0.09738584995205199, w2 = -0.00016077870774933352\n",
            "Epoch 28: w1 = -0.08332808286806326, w2 = -9.853533436149282e-05\n",
            "Epoch 29: w1 = -0.0712995143539851, w2 = 5.08287535662189e-05\n",
            "Epoch 30: w1 = -0.061007259235706914, w2 = 6.451629325061208e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSProp"
      ],
      "metadata": {
        "id": "DEgBrSLKgmS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(W):\n",
        "  w1, w2 = W\n",
        "  dw1 = 0.2 * w1\n",
        "  dw2 = 4 * w2\n",
        "  dW = np.array([dw1, dw2])\n",
        "  return dW\n",
        "\n",
        "\n",
        "def RMSProp(W, dW, lr, S, gamma):\n",
        "  epsilon = 1e-6\n",
        "  S = gamma * S + (1 - gamma) * dW ** 2\n",
        "  adapt_lr = lr / np.sqrt(S + epsilon)\n",
        "  W = W - adapt_lr * dW\n",
        "  return W, S\n",
        "\n",
        "\n",
        "def train_p3(optimizer, lr, epoch):\n",
        "  W = np.array([-5, -2], dtype=np.float32)\n",
        "  S = np.array([0, 0], dtype=np.float32)\n",
        "  results = [W]\n",
        "  for i in range(epoch):\n",
        "    dW = df_w(W)\n",
        "    W, S = optimizer(W, dW, lr, S, 0.9)\n",
        "    results.append(W)\n",
        "    print(f'Epoch {i + 1}: w1 = {W[0]}, w2 = {W[1]}')\n",
        "  return results"
      ],
      "metadata": {
        "id": "bcL1lSXqgnWA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_RMSProp = train_p3(RMSProp, 0.3, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siehB3m5gqVA",
        "outputId": "42c8c8b2-ff40-41fe-acb2-cce0416e66b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w1 = -4.051321445330401, w2 = -1.0513167760653601\n",
            "Epoch 2: w1 = -3.435197540710313, w2 = -0.59152342591607\n",
            "Epoch 3: w1 = -2.9589369293489796, w2 = -0.32943940499816177\n",
            "Epoch 4: w1 = -2.5654628900149308, w2 = -0.1775648185723558\n",
            "Epoch 5: w1 = -2.22920552377513, w2 = -0.09163256127358084\n",
            "Epoch 6: w1 = -1.9362675156207105, w2 = -0.044944986580951356\n",
            "Epoch 7: w1 = -1.6781768574274967, w2 = -0.020814229601575286\n",
            "Epoch 8: w1 = -1.4493498477990567, w2 = -0.009035585595074875\n",
            "Epoch 9: w1 = -1.245881993508816, w2 = -0.003645905472988451\n",
            "Epoch 10: w1 = -1.0649030085077547, w2 = -0.0013535098945501255\n",
            "Epoch 11: w1 = -0.9042022597717997, w2 = -0.00045644443087383875\n",
            "Epoch 12: w1 = -0.7619964948529878, w2 = -0.0001375629281105624\n",
            "Epoch 13: w1 = -0.6367784991349715, w2 = -3.62601019486888e-05\n",
            "Epoch 14: w1 = -0.5272152373016314, w2 = -8.113374556116922e-06\n",
            "Epoch 15: w1 = -0.4320785049217716, w2 = -1.47473411837664e-06\n",
            "Epoch 16: w1 = -0.3501985066951055, w2 = -2.0278399084030024e-07\n",
            "Epoch 17: w1 = -0.2804346489488741, w2 = -1.842311869185962e-08\n",
            "Epoch 18: w1 = -0.22165983448068366, w2 = -7.677427476337743e-10\n",
            "Epoch 19: w1 = -0.1727555124949996, w2 = 7.804519979765185e-12\n",
            "Epoch 20: w1 = -0.1326151335054371, w2 = -5.057948000728944e-13\n",
            "Epoch 21: w1 = -0.1001537791926479, w2 = 6.191235006401474e-14\n",
            "Epoch 22: w1 = -0.07432177081518393, w2 = -1.1337378100969915e-14\n",
            "Epoch 23: w1 = -0.0541201278463423, w2 = 2.801667022532451e-15\n",
            "Epoch 24: w1 = -0.03861591574158471, w2 = -8.813411907873668e-16\n",
            "Epoch 25: w1 = -0.02695580661604302, w2 = 3.3992111677872574e-16\n",
            "Epoch 26: w1 = -0.018376563327480284, w2 = -1.565817306327013e-16\n",
            "Epoch 27: w1 = -0.012211609286904186, w2 = 8.449949845599074e-17\n",
            "Epoch 28: w1 = -0.007893317940141477, w2 = -5.2637659507219454e-17\n",
            "Epoch 29: w1 = -0.004951102607005797, w2 = 3.741079952361897e-17\n",
            "Epoch 30: w1 = -0.003005770808471415, w2 = -3.0050608426117974e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adam"
      ],
      "metadata": {
        "id": "5No1FmN2gsEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_w(W):\n",
        "  w1, w2 = W\n",
        "  dw1 = 0.2 * w1\n",
        "  dw2 = 4 * w2\n",
        "  dW = np.array([dw1, dw2])\n",
        "  return dW\n",
        "\n",
        "\n",
        "def adam(W, dW, lr, V, S, t, beta1=0.9, beta2=0.999):\n",
        "  epsilon = 1e-6\n",
        "  V = beta1 * V + (1 - beta1) * dW\n",
        "  S = beta2 * S + (1 - beta2) * (dW ** 2)\n",
        "  v_corr = V / (1 - beta1**t)\n",
        "  s_corr = S / (1 - beta2**t)\n",
        "  W = W - lr * (v_corr / (np.sqrt(s_corr) + epsilon))\n",
        "  return W, V, S\n",
        "\n",
        "\n",
        "def train_p4(optimizer, lr, epochs):\n",
        "  W = np.array([-5, -2], dtype=np.float32)\n",
        "  V = np.array([0, 0], dtype=np.float32)\n",
        "  S = np.array([0, 0], dtype=np.float32)\n",
        "  results = [W]\n",
        "  for i in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W, V, S = optimizer(W, dW, lr, V, S, i+1)\n",
        "    results.append(W)\n",
        "    print(f'Epoch {i + 1}: w1 = {W[0]}, w2 = {W[1]}')\n",
        "  return results"
      ],
      "metadata": {
        "id": "EesWdRmGgs2q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_adam = train_p4(adam, 0.2, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg8TpF5rgyMF",
        "outputId": "5a2d8675-333c-4390-ddab-60cf478036c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w1 = -4.8000001999998, w2 = -1.8000000249999968\n",
            "Epoch 2: w1 = -4.600254779434054, w2 = -1.6008245063697515\n",
            "Epoch 3: w1 = -4.400948476628311, w2 = -1.4031726206945152\n",
            "Epoch 4: w1 = -4.2022776366594705, w2 = -1.2078782223488431\n",
            "Epoch 5: w1 = -4.004450327821214, w2 = -1.015927446346848\n",
            "Epoch 6: w1 = -3.807686378997748, w2 = -0.8284730661322335\n",
            "Epoch 7: w1 = -3.6122173226091405, w2 = -0.6468415893870743\n",
            "Epoch 8: w1 = -3.4182862261081466, w2 = -0.4725276521059605\n",
            "Epoch 9: w1 = -3.2261473934546006, w2 = -0.3071693439456018\n",
            "Epoch 10: w1 = -3.036065916693978, w2 = -0.15249855183024877\n",
            "Epoch 11: w1 = -2.848317056874701, w2 = -0.010263256257146358\n",
            "Epoch 12: w1 = -2.663185433233414, w2 = 0.11787552325788148\n",
            "Epoch 13: w1 = -2.4809640000598776, w2 = 0.23046161354014214\n",
            "Epoch 14: w1 = -2.301952792136848, w2 = 0.32635870212860313\n",
            "Epoch 15: w1 = -2.126457422346911, w2 = 0.404841946592144\n",
            "Epoch 16: w1 = -1.9547873191379472, w2 = 0.4656496111781283\n",
            "Epoch 17: w1 = -1.7872536971852042, w2 = 0.5089879942906521\n",
            "Epoch 18: w1 = -1.6241672618485605, w2 = 0.5354944186093761\n",
            "Epoch 19: w1 = -1.4658356568594801, w2 = 0.5461714353253214\n",
            "Epoch 20: w1 = -1.3125606749085668, w2 = 0.542308123651489\n",
            "Epoch 21: w1 = -1.1646352621090186, w2 = 0.525402057025836\n",
            "Epoch 22: w1 = -1.0223403591835882, w2 = 0.49709059750650125\n",
            "Epoch 23: w1 = -0.8859416340040318, w2 = 0.45909510392164704\n",
            "Epoch 24: w1 = -0.7556861710156256, w2 = 0.41317780979225216\n",
            "Epoch 25: w1 = -0.6317991922453273, w2 = 0.3611089043383209\n",
            "Epoch 26: w1 = -0.5144808911644565, w2 = 0.3046404828404245\n",
            "Epoch 27: w1 = -0.4039034638949965, w2 = 0.24548408953076975\n",
            "Epoch 28: w1 = -0.3002084215436735, w2 = 0.1852891816627655\n",
            "Epoch 29: w1 = -0.20350426252765802, w2 = 0.12562074192655373\n",
            "Epoch 30: w1 = -0.11386457466501028, w2 = 0.06793529370574175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ADOPT"
      ],
      "metadata": {
        "id": "3nZon3p-hsOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mypy: allow-untyped-decorators\n",
        "# mypy: allow-untyped-defs\n",
        "from typing import cast, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.optim.optimizer import (\n",
        "    _capturable_doc,\n",
        "    _default_to_fused_or_foreach,\n",
        "    _device_dtype_check_for_fused,\n",
        "    _differentiable_doc,\n",
        "    _disable_dynamo_if_unsupported,\n",
        "    _foreach_doc,\n",
        "    _fused_doc,\n",
        "    _get_capturable_supported_devices,\n",
        "    _get_scalar_dtype,\n",
        "    _get_value,\n",
        "    _maximize_doc,\n",
        "    _stack_if_compiling,\n",
        "    _use_grad_for_differentiable,\n",
        "    _view_as_real,\n",
        "    DeviceDict,\n",
        "    Optimizer,\n",
        "    ParamsT,\n",
        ")\n",
        "\n",
        "\n",
        "__all__ = [\"ADOPT\", \"adopt\"]\n",
        "\n",
        "\n",
        "class ADOPT(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "        lr: Union[float, Tensor] = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.9999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        decoupled: bool = False,\n",
        "        *,\n",
        "        foreach: Optional[bool] = None,\n",
        "        maximize: bool = False,\n",
        "        capturable: bool = False,\n",
        "        differentiable: bool = False,\n",
        "        fused: Optional[bool] = None,\n",
        "    ):\n",
        "        if isinstance(lr, Tensor):\n",
        "            if foreach and not capturable:\n",
        "                raise ValueError(\n",
        "                    \"lr as a Tensor is not supported for capturable=False and foreach=True\"\n",
        "                )\n",
        "            if lr.numel() != 1:\n",
        "                raise ValueError(\"Tensor lr must be 1-element\")\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "            decoupled=decoupled,\n",
        "            maximize=maximize,\n",
        "            foreach=foreach,\n",
        "            capturable=capturable,\n",
        "            differentiable=differentiable,\n",
        "            fused=fused,\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        if fused:\n",
        "            # TODO: support fused\n",
        "            raise RuntimeError(\"`fused` is not currently supported\")\n",
        "\n",
        "            if differentiable:\n",
        "                raise RuntimeError(\"`fused` does not support `differentiable`\")\n",
        "            self._step_supports_amp_scaling = True\n",
        "            # TODO(crcrpar): [low prec params & their higher prec copy]\n",
        "            # Support AMP with FP16/BF16 model params which would need\n",
        "            # higher prec copy of params to do update math in higher prec to\n",
        "            # alleviate the loss of information.\n",
        "            if foreach:\n",
        "                raise RuntimeError(\"`fused` and `foreach` cannot be `True` together.\")\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault(\"maximize\", False)\n",
        "            group.setdefault(\"foreach\", None)\n",
        "            group.setdefault(\"capturable\", False)\n",
        "            group.setdefault(\"differentiable\", False)\n",
        "            fused = group.setdefault(\"fused\", None)\n",
        "            for p in group[\"params\"]:\n",
        "                p_state = self.state.get(p, [])\n",
        "                if len(p_state) != 0 and not torch.is_tensor(p_state[\"step\"]):\n",
        "                    step_val = float(p_state[\"step\"])\n",
        "                    p_state[\"step\"] = (\n",
        "                        torch.tensor(\n",
        "                            step_val,\n",
        "                            dtype=_get_scalar_dtype(is_fused=fused),\n",
        "                            device=p.device,\n",
        "                        )\n",
        "                        if group[\"capturable\"] or group[\"fused\"]\n",
        "                        else torch.tensor(step_val, dtype=_get_scalar_dtype())\n",
        "                    )\n",
        "\n",
        "    def _init_group(\n",
        "        self,\n",
        "        group,\n",
        "        params_with_grad,\n",
        "        grads,\n",
        "        exp_avgs,\n",
        "        exp_avg_sqs,\n",
        "        state_steps,\n",
        "    ):\n",
        "        has_complex = False\n",
        "        for p in group[\"params\"]:\n",
        "            if p.grad is not None:\n",
        "                has_complex |= torch.is_complex(p)\n",
        "                params_with_grad.append(p)\n",
        "                if p.grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        \"ADOPT does not support sparse gradients\"\n",
        "                    )\n",
        "                grads.append(p.grad)\n",
        "\n",
        "                state = self.state[p]\n",
        "                # Lazy state initialization\n",
        "                if len(state) == 0:\n",
        "                    if group[\"fused\"]:\n",
        "                        _device_dtype_check_for_fused(p)\n",
        "                    # note(crcrpar): [special device hosting for step]\n",
        "                    # Deliberately host `step` on CPU if both capturable and fused are off.\n",
        "                    # This is because kernel launches are costly on CUDA and XLA.\n",
        "                    state[\"step\"] = (\n",
        "                        torch.zeros(\n",
        "                            (),\n",
        "                            dtype=_get_scalar_dtype(is_fused=group[\"fused\"]),\n",
        "                            device=p.device,\n",
        "                        )\n",
        "                        if group[\"capturable\"] or group[\"fused\"]\n",
        "                        else torch.tensor(0.0, dtype=_get_scalar_dtype())\n",
        "                    )\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "\n",
        "                exp_avgs.append(state[\"exp_avg\"])\n",
        "                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n",
        "\n",
        "                if group[\"differentiable\"] and state[\"step\"].requires_grad:\n",
        "                    raise RuntimeError(\n",
        "                        \"`requires_grad` is not supported for `step` in differentiable mode\"\n",
        "                    )\n",
        "\n",
        "                # Foreach without capturable does not support a tensor lr\n",
        "                if (\n",
        "                    group[\"foreach\"]\n",
        "                    and torch.is_tensor(group[\"lr\"])\n",
        "                    and not group[\"capturable\"]\n",
        "                ):\n",
        "                    raise RuntimeError(\n",
        "                        \"lr as a Tensor is not supported for capturable=False and foreach=True\"\n",
        "                    )\n",
        "\n",
        "                state_steps.append(state[\"step\"])\n",
        "        return has_complex\n",
        "\n",
        "    @_use_grad_for_differentiable\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Perform a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (Callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        self._cuda_graph_capture_health_check()\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad: List[Tensor] = []\n",
        "            grads: List[Tensor] = []\n",
        "            exp_avgs: List[Tensor] = []\n",
        "            exp_avg_sqs: List[Tensor] = []\n",
        "            state_steps: List[Tensor] = []\n",
        "            beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "            has_complex = self._init_group(\n",
        "                group,\n",
        "                params_with_grad,\n",
        "                grads,\n",
        "                exp_avgs,\n",
        "                exp_avg_sqs,\n",
        "                state_steps,\n",
        "            )\n",
        "\n",
        "            adopt(\n",
        "                params_with_grad,\n",
        "                grads,\n",
        "                exp_avgs,\n",
        "                exp_avg_sqs,\n",
        "                state_steps,\n",
        "                has_complex=has_complex,\n",
        "                beta1=beta1,\n",
        "                beta2=beta2,\n",
        "                lr=group[\"lr\"],\n",
        "                weight_decay=group[\"weight_decay\"],\n",
        "                decoupled=group[\"decoupled\"],\n",
        "                eps=group[\"eps\"],\n",
        "                maximize=group[\"maximize\"],\n",
        "                foreach=group[\"foreach\"],\n",
        "                capturable=group[\"capturable\"],\n",
        "                differentiable=group[\"differentiable\"],\n",
        "                fused=group[\"fused\"],\n",
        "                grad_scale=getattr(self, \"grad_scale\", None),\n",
        "                found_inf=getattr(self, \"found_inf\", None),\n",
        "            )\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def _single_tensor_adopt(\n",
        "    params: List[Tensor],\n",
        "    grads: List[Tensor],\n",
        "    exp_avgs: List[Tensor],\n",
        "    exp_avg_sqs: List[Tensor],\n",
        "    state_steps: List[Tensor],\n",
        "    grad_scale: Optional[Tensor],\n",
        "    found_inf: Optional[Tensor],\n",
        "    *,\n",
        "    has_complex: bool,\n",
        "    beta1: float,\n",
        "    beta2: float,\n",
        "    lr: Union[float, Tensor],\n",
        "    weight_decay: float,\n",
        "    decoupled: bool,\n",
        "    eps: float,\n",
        "    maximize: bool,\n",
        "    capturable: bool,\n",
        "    differentiable: bool,\n",
        "):\n",
        "    assert grad_scale is None and found_inf is None\n",
        "\n",
        "    if torch.jit.is_scripting():\n",
        "        # this assert is due to JIT being dumb and not realizing that the ops below\n",
        "        # have overloads to handle both float and Tensor lrs, so we just assert it's\n",
        "        # a float since most people using JIT are using floats\n",
        "        assert isinstance(lr, float)\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "        grad = grads[i] if not maximize else -grads[i]\n",
        "        exp_avg = exp_avgs[i]\n",
        "        exp_avg_sq = exp_avg_sqs[i]\n",
        "        step_t = state_steps[i]\n",
        "\n",
        "        # If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]\n",
        "        if not torch._utils.is_compiling() and capturable:\n",
        "            capturable_supported_devices = _get_capturable_supported_devices()\n",
        "            assert (\n",
        "                param.device.type == step_t.device.type\n",
        "                and param.device.type in capturable_supported_devices\n",
        "            ), f\"If capturable=True, params and state_steps must be on supported devices: {capturable_supported_devices}.\"\n",
        "\n",
        "        # update step\n",
        "        step_t += 1\n",
        "\n",
        "        if weight_decay != 0:\n",
        "            if decoupled:\n",
        "                param.add_(param, alpha=-lr*weight_decay)\n",
        "            else:\n",
        "                grad = grad.add(param, alpha=weight_decay)\n",
        "\n",
        "        if torch.is_complex(param):\n",
        "            grad = torch.view_as_real(grad)\n",
        "            if exp_avg is not None:\n",
        "                exp_avg = torch.view_as_real(exp_avg)\n",
        "            if exp_avg_sq is not None:\n",
        "                exp_avg_sq = torch.view_as_real(exp_avg_sq)\n",
        "            param = torch.view_as_real(param)\n",
        "\n",
        "        step = step_t if capturable or differentiable else _get_value(step_t)\n",
        "        if step == 1:\n",
        "            exp_avg_sq.addcmul_(grad, grad.conj())\n",
        "            continue\n",
        "\n",
        "        denom = torch.clamp(exp_avg_sq.sqrt(), eps)\n",
        "        if step == 2:\n",
        "            exp_avg.addcdiv_(grad, denom)\n",
        "        else:\n",
        "            exp_avg.mul_(beta1).addcdiv_(grad, denom, value=1 - beta1)\n",
        "\n",
        "        param.add_(exp_avg, alpha=-lr)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
        "\n",
        "\n",
        "def _multi_tensor_adopt(\n",
        "    params: List[Tensor],\n",
        "    grads: List[Tensor],\n",
        "    exp_avgs: List[Tensor],\n",
        "    exp_avg_sqs: List[Tensor],\n",
        "    state_steps: List[Tensor],\n",
        "    grad_scale: Optional[Tensor],\n",
        "    found_inf: Optional[Tensor],\n",
        "    *,\n",
        "    has_complex: bool,\n",
        "    beta1: float,\n",
        "    beta2: float,\n",
        "    lr: Union[float, Tensor],\n",
        "    weight_decay: float,\n",
        "    decoupled: bool,\n",
        "    eps: float,\n",
        "    maximize: bool,\n",
        "    capturable: bool,\n",
        "    differentiable: bool,\n",
        "):\n",
        "    if len(params) == 0:\n",
        "        return\n",
        "\n",
        "    if isinstance(lr, Tensor) and not capturable:\n",
        "        raise RuntimeError(\n",
        "            \"lr as a Tensor is not supported for capturable=False and foreach=True\"\n",
        "        )\n",
        "\n",
        "    # If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]\n",
        "    if not torch._utils.is_compiling() and capturable:\n",
        "        capturable_supported_devices = _get_capturable_supported_devices(\n",
        "            supports_xla=False\n",
        "        )\n",
        "        assert all(\n",
        "            p.device.type == step.device.type\n",
        "            and p.device.type in capturable_supported_devices\n",
        "            for p, step in zip(params, state_steps)\n",
        "        ), f\"If capturable=True, params and state_steps must be on supported devices: {capturable_supported_devices}.\"\n",
        "\n",
        "    assert grad_scale is None and found_inf is None\n",
        "\n",
        "    assert not differentiable, \"_foreach ops don't support autograd\"\n",
        "\n",
        "    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(\n",
        "        [params, grads, exp_avgs, exp_avg_sqs, state_steps]  # type: ignore[list-item]\n",
        "    )\n",
        "    for (\n",
        "        device_params_,\n",
        "        device_grads_,\n",
        "        device_exp_avgs_,\n",
        "        device_exp_avg_sqs_,\n",
        "        device_state_steps_,\n",
        "    ), _ in grouped_tensors.values():\n",
        "        device_params = cast(List[Tensor], device_params_)\n",
        "        device_grads = cast(List[Tensor], device_grads_)\n",
        "        device_exp_avgs = cast(List[Tensor], device_exp_avgs_)\n",
        "        device_exp_avg_sqs = cast(List[Tensor], device_exp_avg_sqs_)\n",
        "        device_state_steps = cast(List[Tensor], device_state_steps_)\n",
        "\n",
        "        # Handle complex parameters\n",
        "        if has_complex:\n",
        "            _view_as_real(\n",
        "                device_params, device_grads, device_exp_avgs, device_exp_avg_sqs\n",
        "            )\n",
        "\n",
        "        if maximize:\n",
        "            device_grads = torch._foreach_neg(device_grads)  # type: ignore[assignment]\n",
        "\n",
        "        # Update steps\n",
        "        # If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\n",
        "        # and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\n",
        "        # wrapped it once now. The alpha is required to assure we go to the right overload.\n",
        "        if not torch._utils.is_compiling() and device_state_steps[0].is_cpu:\n",
        "            torch._foreach_add_(\n",
        "                device_state_steps, torch.tensor(1.0, device=\"cpu\"), alpha=1.0\n",
        "            )\n",
        "        else:\n",
        "            torch._foreach_add_(device_state_steps, 1)\n",
        "\n",
        "        if weight_decay != 0:\n",
        "            if decoupled:\n",
        "                torch._foreach_add_(device_params, device_params, alpha=-lr*weight_decay)\n",
        "            else:\n",
        "                # Re-use the intermediate memory (device_grads) already allocated for maximize\n",
        "                if maximize:\n",
        "                    torch._foreach_add_(device_grads, device_params, alpha=weight_decay)\n",
        "                else:\n",
        "                    device_grads = torch._foreach_add(  # type: ignore[assignment]\n",
        "                        device_grads, device_params, alpha=weight_decay\n",
        "                    )\n",
        "\n",
        "        if device_state_steps[0] == 1:\n",
        "            torch._foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads)\n",
        "            continue\n",
        "\n",
        "        exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
        "        exp_avg_sq_sqrt = torch._foreach_maximum(exp_avg_sq_sqrt, eps)\n",
        "\n",
        "        if device_state_steps[0] == 2:\n",
        "            torch._foreach_addcdiv_(device_exp_avgs, device_grads, exp_avg_sq_sqrt)\n",
        "        else:\n",
        "            torch._foreach_mul_(device_exp_avgs, beta1)\n",
        "            torch._foreach_addcdiv_(\n",
        "                device_exp_avgs, device_grads, exp_avg_sq_sqrt, value=1 - beta1\n",
        "            )\n",
        "\n",
        "        torch._foreach_add_(device_params, device_exp_avgs, alpha=-lr)\n",
        "        torch._foreach_mul_(device_exp_avg_sqs, beta2)\n",
        "        torch._foreach_addcmul_(\n",
        "            device_exp_avg_sqs, device_grads, device_grads, value=1 - beta2\n",
        "        )\n",
        "\n",
        "\n",
        "@_disable_dynamo_if_unsupported(single_tensor_fn=_single_tensor_adopt)\n",
        "def adopt(\n",
        "    params: List[Tensor],\n",
        "    grads: List[Tensor],\n",
        "    exp_avgs: List[Tensor],\n",
        "    exp_avg_sqs: List[Tensor],\n",
        "    state_steps: List[Tensor],\n",
        "    # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
        "    # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
        "    foreach: Optional[bool] = None,\n",
        "    capturable: bool = False,\n",
        "    differentiable: bool = False,\n",
        "    fused: Optional[bool] = None,\n",
        "    grad_scale: Optional[Tensor] = None,\n",
        "    found_inf: Optional[Tensor] = None,\n",
        "    has_complex: bool = False,\n",
        "    *,\n",
        "    beta1: float,\n",
        "    beta2: float,\n",
        "    lr: Union[float, Tensor],\n",
        "    weight_decay: float,\n",
        "    decoupled: bool,\n",
        "    eps: float,\n",
        "    maximize: bool,\n",
        "):\n",
        "    r\"\"\"Functional API that performs ADOPT algorithm computation.\n",
        "\n",
        "    \"\"\"\n",
        "    # Respect when the user inputs False/True for foreach or fused. We only want to change\n",
        "    # the default when neither have been user-specified. Note that we default to foreach\n",
        "    # and pass False to use_fused. This is not a mistake--we want to give the fused impl\n",
        "    # bake-in time before making it the default, even if it is typically faster.\n",
        "    if fused is None and foreach is None:\n",
        "        _, foreach = _default_to_fused_or_foreach(\n",
        "            params, differentiable, use_fused=False\n",
        "        )\n",
        "        # Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\n",
        "        if foreach and isinstance(lr, Tensor) and not capturable:\n",
        "            foreach = False\n",
        "    if fused is None:\n",
        "        fused = False\n",
        "    if foreach is None:\n",
        "        foreach = False\n",
        "\n",
        "    # this check is slow during compilation, so we skip it\n",
        "    # if it's strictly needed we can add this check back in dynamo\n",
        "    if not torch._utils.is_compiling() and not all(\n",
        "        isinstance(t, torch.Tensor) for t in state_steps\n",
        "    ):\n",
        "        raise RuntimeError(\n",
        "            \"API has changed, `state_steps` argument must contain a list of singleton tensors\"\n",
        "        )\n",
        "\n",
        "    if foreach and torch.jit.is_scripting():\n",
        "        raise RuntimeError(\"torch.jit.script not supported with foreach optimizers\")\n",
        "    if fused and torch.jit.is_scripting():\n",
        "        raise RuntimeError(\"torch.jit.script not supported with fused optimizers\")\n",
        "\n",
        "    if fused and not torch.jit.is_scripting():\n",
        "        func = _fused_adopt\n",
        "    elif foreach and not torch.jit.is_scripting():\n",
        "        func = _multi_tensor_adopt\n",
        "    else:\n",
        "        func = _single_tensor_adopt\n",
        "\n",
        "    func(\n",
        "        params,\n",
        "        grads,\n",
        "        exp_avgs,\n",
        "        exp_avg_sqs,\n",
        "        state_steps,\n",
        "        has_complex=has_complex,\n",
        "        beta1=beta1,\n",
        "        beta2=beta2,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        decoupled=decoupled,\n",
        "        eps=eps,\n",
        "        maximize=maximize,\n",
        "        capturable=capturable,\n",
        "        differentiable=differentiable,\n",
        "        grad_scale=grad_scale,\n",
        "        found_inf=found_inf,\n",
        "    )"
      ],
      "metadata": {
        "id": "WZUdfFZkhuAr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def main():\n",
        "    # Tạo một mô hình ví dụ\n",
        "    model = nn.Linear(10, 2)\n",
        "    # Sử dụng ADOPT làm optimizer với learning rate lr = 0.2\n",
        "    optimizer = ADOPT(model.parameters(), lr=0.2)\n",
        "\n",
        "    # Tạo một loss function\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Dữ liệu giả lập\n",
        "    inputs = torch.randn(5, 10)\n",
        "    targets = torch.randn(5, 2)\n",
        "\n",
        "    # Tiến hành huấn luyện trong 30 vòng lặp\n",
        "    for epoch in range(30):  # Chỉnh sửa số lượng epoch thành 30\n",
        "        optimizer.zero_grad()  # Đặt gradient về 0\n",
        "        outputs = model(inputs)  # Tiến hành truyền qua mô hình\n",
        "        loss = criterion(outputs, targets)  # Tính toán loss\n",
        "        loss.backward()  # Lan truyền gradient\n",
        "        optimizer.step()  # Cập nhật tham số\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ROZpKYviosD",
        "outputId": "bd6a9592-4c74-4684-f712-fe68a00f24d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9868853688240051\n",
            "Epoch 2, Loss: 0.9868853688240051\n",
            "Epoch 3, Loss: 0.4614774286746979\n",
            "Epoch 4, Loss: 0.7535218000411987\n",
            "Epoch 5, Loss: 1.0737407207489014\n",
            "Epoch 6, Loss: 1.1585156917572021\n",
            "Epoch 7, Loss: 1.2143940925598145\n",
            "Epoch 8, Loss: 1.4108127355575562\n",
            "Epoch 9, Loss: 1.6695811748504639\n",
            "Epoch 10, Loss: 1.8374748229980469\n",
            "Epoch 11, Loss: 1.864385962486267\n",
            "Epoch 12, Loss: 1.777327299118042\n",
            "Epoch 13, Loss: 1.593823790550232\n",
            "Epoch 14, Loss: 1.2887804508209229\n",
            "Epoch 15, Loss: 0.86248379945755\n",
            "Epoch 16, Loss: 0.44315099716186523\n",
            "Epoch 17, Loss: 0.2067890465259552\n",
            "Epoch 18, Loss: 0.17975711822509766\n",
            "Epoch 19, Loss: 0.22670495510101318\n",
            "Epoch 20, Loss: 0.22927939891815186\n",
            "Epoch 21, Loss: 0.20042526721954346\n",
            "Epoch 22, Loss: 0.21419289708137512\n",
            "Epoch 23, Loss: 0.28603464365005493\n",
            "Epoch 24, Loss: 0.3671693205833435\n",
            "Epoch 25, Loss: 0.41217952966690063\n",
            "Epoch 26, Loss: 0.40928274393081665\n",
            "Epoch 27, Loss: 0.3710615038871765\n",
            "Epoch 28, Loss: 0.31296306848526\n",
            "Epoch 29, Loss: 0.23903706669807434\n",
            "Epoch 30, Loss: 0.15595270693302155\n"
          ]
        }
      ]
    }
  ]
}